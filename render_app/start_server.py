#!/usr/bin/env python3
"""
BTC Chart Server Startup Script
================================

This script starts the BTC data logger server and provides utilities for testing.
It includes sample data generation for development and testing purposes.
"""

import os
import sys
import json
import time
import threading
from datetime import datetime, timedelta
import pandas as pd

# Add current directory to path so we can import our modules
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from logger import app, log_data
from process_data import process_csv_to_json

DATA_FOLDER = "render_app/data"

def generate_sample_data(hours=48, interval_minutes=1):
    """Generate sample BTC data for testing the charts"""
    print(f"ğŸ”§ Generating {hours} hours of sample data...")
    
    # Create realistic sample data
    start_time = datetime.utcnow() - timedelta(hours=hours)
    data_points = []
    
    # Starting values
    base_price = 65000
    base_spread = 0.0005
    
    for i in range(int(hours * 60 / interval_minutes)):
        timestamp = start_time + timedelta(minutes=i * interval_minutes)
        
        # Add some realistic price movement
        price_change = (hash(str(timestamp)) % 2000 - 1000) / 100  # Random-ish price movement
        price = base_price + price_change + (i * 0.1)  # Slight upward trend
        
        # Realistic spread data
        spread_base = base_spread + (hash(str(timestamp + timedelta(seconds=30))) % 100 - 50) / 1000000
        spread_pct = spread_base * 100
        
        data_point = {
            "timestamp": timestamp.isoformat(),
            "asset": "BTC-USD",
            "exchange": "Coinbase",
            "price": round(price, 2),
            "bid": round(price - (price * spread_base / 2), 2),
            "ask": round(price + (price * spread_base / 2), 2),
            "spread": round(price * spread_base, 2),
            "volume": 50 + (hash(str(timestamp)) % 100),  # Random volume
            "spread_avg_L20": round(price * spread_base, 6),
            "spread_avg_L20_pct": round(spread_pct, 8)
        }
        data_points.append(data_point)
    
    # Group by 8-hour blocks (matching your logger's file rotation)
    grouped_data = {}
    for point in data_points:
        dt = datetime.fromisoformat(point["timestamp"])
        hour_block = (dt.hour // 8) * 8
        date_str = dt.strftime("%Y-%m-%d")
        filename = f"{date_str}_{hour_block:02d}.csv"
        
        if filename not in grouped_data:
            grouped_data[filename] = []
        grouped_data[filename].append(point)
    
    # Write CSV files
    os.makedirs(DATA_FOLDER, exist_ok=True)
    
    for filename, points in grouped_data.items():
        filepath = os.path.join(DATA_FOLDER, filename)
        
        # Convert to DataFrame and save
        df = pd.DataFrame(points)
        df.to_csv(filepath, index=False)
        print(f"ğŸ“ Created: {filepath} ({len(points)} records)")
    
    print(f"âœ… Generated {len(data_points)} sample data points across {len(grouped_data)} files")
    return len(data_points)

def test_data_processing():
    """Test the data processing pipeline"""
    print("ğŸ§ª Testing data processing pipeline...")
    try:
        process_csv_to_json()
        print("âœ… Data processing completed successfully!")
        
        # Check generated files
        files_to_check = ['recent.json', 'historical.json', 'metadata.json', 'index.json']
        for filename in files_to_check:
            filepath = os.path.join(DATA_FOLDER, filename)
            if os.path.exists(filepath):
                file_size = os.path.getsize(filepath)
                print(f"ğŸ“„ {filename}: {file_size:,} bytes")
            else:
                print(f"âŒ Missing: {filename}")
                
    except Exception as e:
        print(f"âŒ Data processing failed: {e}")
        return False
    
    return True

def start_logger_thread():
    """Start the data logger in a separate thread"""
    print("ğŸš€ Starting data logger thread...")
    logger_thread = threading.Thread(target=log_data, daemon=True)
    logger_thread.start()
    print("âœ… Data logger started!")
    return logger_thread

def show_endpoints():
    """Display available API endpoints"""
    print("\n" + "="*60)
    print("ğŸŒ AVAILABLE API ENDPOINTS")
    print("="*60)
    
    endpoints = {
        "Chart Data (Recommended)": [
            "/recent.json - Last 24 hours (fast loading)",
            "/historical.json - Complete dataset (full history)",
            "/metadata.json - Dataset information",
            "/index.json - Data source index"
        ],
        "Raw Data": [
            "/data.csv - Current CSV file",
            "/csv-list - List all CSV files",
            "/csv/<filename> - Download specific CSV"
        ],
        "Filtered Data": [
            "/chart-data?limit=1000 - Limited data points",
            "/chart-data?start_date=2025-01-01 - Date filtered"
        ],
        "Daily JSON": [
            "/json/output_<YYYY-MM-DD>.json - Daily data",
            "/output-latest.json - Latest daily data"
        ]
    }
    
    for category, urls in endpoints.items():
        print(f"\nğŸ“‹ {category}:")
        for url in urls:
            print(f"   â€¢ {url}")
    
    print(f"\nğŸ”— Base URL: http://localhost:10000")
    print("ğŸ“Š Chart Files:")
    print("   â€¢ improved_btc_chart.html - New enhanced chart")
    print("   â€¢ tradingview_style_chart.html - TradingView style")
    print("   â€¢ chart_config_example.html - Basic example")
    print("="*60)

def main():
    """Main startup function"""
    print("ğŸš€ BTC Chart Server Startup")
    print("="*40)
    
    # Check if we have any data
    csv_files = [f for f in os.listdir(DATA_FOLDER) if f.endswith('.csv')] if os.path.exists(DATA_FOLDER) else []
    
    if not csv_files:
        print("ğŸ“Š No data found. Generating sample data...")
        generate_sample_data(hours=48)  # 2 days of data
        test_data_processing()
    else:
        print(f"ğŸ“ Found {len(csv_files)} existing CSV files")
        print("ğŸ”„ Processing existing data...")
        test_data_processing()
    
    # Start the data logger
    start_logger_thread()
    
    # Wait a moment for everything to initialize
    time.sleep(2)
    
    # Show available endpoints
    show_endpoints()
    
    print("\nğŸ¯ QUICK START:")
    print("1. Open http://localhost:10000 in your browser to see API status")
    print("2. Open render_app/improved_btc_chart.html to view the enhanced chart")
    print("3. The chart will auto-detect localhost and use the correct API endpoint")
    print("4. Data will update automatically every 30 seconds when auto-refresh is enabled")
    
    print("\nğŸš€ Starting Flask server on http://localhost:10000...")
    
    try:
        # Start the Flask app
        app.run(host="0.0.0.0", port=10000, debug=False)
    except KeyboardInterrupt:
        print("\nâ¹ï¸ Server stopped by user")
    except Exception as e:
        print(f"\nâŒ Server error: {e}")

if __name__ == "__main__":
    main()